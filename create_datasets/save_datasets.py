#!/usr/bin/env python3.5
import matplotlib_handle_display  # Must be imported before anything matplotlib related
import argparse
import numpy as np
import os
import pickle
from matplotlib import pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import scipy.stats as stats
from scipy.interpolate import RegularGridInterpolator
from collections import Counter
from parse_volumes_dataset import plot_pet_slice
from generate_dataset import get_current_time


"""
This code should be run after parse_volumes_dataset.py
and before easy_experiments_runner.py
It gets the .pkl files generated by parse_volumes_dataset.py
and saves a single .npz file. It may perform some transformation
on the data too, like rotating it to increase the number of samples,
or separate the 3D volumes in 3 channels 2D slices.
"""


np.random.seed(123)  # for reproducibility


def save_plt_figures_to_pdf(filename, figs=None, dpi=200):
    """Save all matplotlib figures in a single PDF file."""
    dirname = os.path.dirname(filename)
    try:
        os.makedirs(dirname)
    except OSError:
        pass
    pp = PdfPages(filename)
    if figs is None:
        figs = [plt.figure(n) for n in plt.get_fignums()]
    for fig in figs:
        fig.savefig(pp, format='pdf')
    pp.close()
    print("PDF file saved in '{}'.".format(filename))


def save_data(x_set, y_set, patients, number, suffix=""):
    """Save information from x_set and y_set. Old method (naive one)."""
    if number == 1 or number == 2:
        # Split volumes in slices_per_sample (3) layers images
        counter = 0
        rotations = 4
        normalize = True
        slices_per_sample = 3
        x_dataset = []
        y_dataset = []
        patients_dataset = []
        for volume, label, patient in zip(x_set, y_set, patients):
            # Normalize values from 0 to 1
            maxv = np.max(volume)
            minv = np.min(volume)
            if normalize:
                volume = (volume - minv) / (maxv - minv)
            for r in range(rotations):
                # If rotations=4, rotate img 4 times (0, 90, 180 and 270 deg) to incr. sample size
                vol = np.rot90(volume, k=r)
                for idx in range(vol.shape[2] - slices_per_sample + 1):
                    image = vol[:, :, idx:idx + slices_per_sample]
                    x_dataset.append(image)
                    y_dataset.append(label)
                    patients_dataset.append(patient)
            counter += 1
            print("{}  {} / {} patients processed".format(get_current_time(microseconds=True),
                                                          counter, len(patients)))
        print("Processing finished! Saving data...")
        # Save data
        x = np.array(x_dataset)
        y = np.array(y_dataset)
        try:
            os.mkdir("data")
        except OSError:
            pass
        full_path = "data/"
        folder_path = full_path + "radiomics{}{}".format(suffix, number)
        file_path = "{}/radiomics{}{}".format(folder_path, suffix, number)
        try:
            os.mkdir(folder_path)
        except OSError:
            pass
        np.savez(file_path, x=x, y=y)
        print("Dataset saved in: '{}.npz'".format(file_path))
        with open("{}_patients.pkl".format(file_path), "wb") as f:
            pickle.dump(patients_dataset, f)
        print("Patients saved in '{}_patients.pkl'.".format(file_path))
    else:
        print("Dataset number '{}' unknown, data was not saved".format(number))


def get_size_mask(mask):
    """Get size box and volume of mask where we can fit all 1s in contour."""
    pixel_shape = mask.shape
    mask_range = [[pixel_shape[0], pixel_shape[1], pixel_shape[2]], [-1, -1, -1]]
    volume = 0
    for xx in range(pixel_shape[0]):
        for yy in range(pixel_shape[1]):
            for zz in range(pixel_shape[2]):
                if mask[xx, yy, zz]:
                    volume += 1
                    mask_range[0][0] = min(mask_range[0][0], xx)
                    mask_range[0][1] = min(mask_range[0][1], yy)
                    mask_range[0][2] = min(mask_range[0][2], zz)
                    mask_range[1][0] = max(mask_range[1][0], xx)
                    mask_range[1][1] = max(mask_range[1][1], yy)
                    mask_range[1][2] = max(mask_range[1][2], zz)
    box_size = np.array(mask_range[1]) - np.array(mask_range[0]) + 1
    return box_size, volume


def get_size_mask_efficiently(mask):
    """Get size box and volume of mask where we can fit all 1s in contour."""
    ones_pos = np.nonzero(mask)
    box_size = np.max(ones_pos, axis=1) - np.min(ones_pos, axis=1) + 1
    volume = len(ones_pos[0])
    return box_size, volume


def plot_histogram(data, title=None, figure=0, subfigure=None, bins=10, xlim=None, show=True,
                   percentages=(0.1, 0.25, 0.75, 0.9), figsize=(8 * 2, 6 * 2), window_title=None,
                   close_all=False):
    """Plot histogram of data."""
    sorted_data = sorted(data)
    # Close and erase all old figures
    if close_all:
        plt.close("all")
    # This is a fitting indeed (draws normal distribution centered at mean)
    fit = stats.norm.pdf(sorted_data, np.mean(sorted_data), np.std(sorted_data))
    fig = plt.figure(figure, figsize=figsize)
    if subfigure is not None:
        fig.add_subplot(subfigure)
    plt.plot(sorted_data, fit, '.-')
    # Use this to draw histogram of the data
    plt.hist(sorted_data, normed=True, bins=bins)
    if xlim is not None:
        plt.xlim(xlim)
    if title is not None:
        plt.title(title)
        fig.canvas.set_window_title("Figure {} - {}".format(figure, title))
    if window_title is not None:
        fig.canvas.set_window_title("Figure {} - {}".format(figure, window_title))
    # Add vertical lines in percentages
    if percentages is not None:
        for i, p in enumerate(percentages):
            pos = int(np.round(len(sorted_data) * p))
            if pos >= len(sorted_data):
                pos = len(sorted_data) - 1
            if p == 0.1 or p == 0.9:
                linestyle = "-."
                linecolor = "#904040"
            elif p == 0.25 or p == 0.75:
                linestyle = "--"
                linecolor = "#409040"
            else:
                linestyle = ":"
                linecolor = "#404090"
            label = None
            if p == 0.1:
                label = "10 % - 90 %"
            elif p == 0.25:
                label = "25 % - 75 %"
            plt.axvline(x=sorted_data[pos], linestyle=linestyle, color=linecolor, lw=1,
                        label=label)
        plt.legend()
        # plt.tight_layout()  # Avoids overlap text and figures
    if show:
        plt.show()


def plot_boxplot(data, title=None, figure=0, subfigure=None, ylim=None, hide_axis_labels=False,
                 window_title=None, show=True, close_all=False):
    """Plot a boxplot (figure where we can easily see median, var and mean) of data."""
    # Close and erase all old figures
    if close_all:
        plt.close("all")
    # Draw boxplot
    fig = plt.figure(figure)
    if subfigure is None:
        subfigure = 111
    ax = fig.add_subplot(subfigure)
    ax.boxplot(data, showmeans=True)
    if ylim is not None:
        plt.ylim(ylim)
    if title is not None:
        plt.title(title)
        fig.canvas.set_window_title("Figure {} - {}".format(figure, title))
    if window_title is not None:
        fig.canvas.set_window_title("Figure {} - {}".format(figure, window_title))
    if hide_axis_labels:
        ax.tick_params(labelleft='off')
    ax.tick_params(labelbottom='off')
    if show:
        plt.show()


def trim_edges(array_sort, sort_method, x_set, y_set, patients, masks, trim_pos=(0.1, 0.9)):
    """Remove top and bottom data (data further from average).

    array_sort: array that determines how data is sorted, which determines what stays or is trimmed
    sort_method: 3 possible values: "slices", "sizes" and "sizes_masks"
    """
    arr = np.array(array_sort)
    num_left = int(np.round(len(array_sort) * trim_pos[0]))
    num_right = int(np.round(len(array_sort) * (1 - trim_pos[1] + trim_pos[0])) - num_left)
    val_left = array_sort[arr.argsort()[num_left - 1]]
    val_right = array_sort[arr.argsort()[-num_right]]
    new_x_set = []
    new_y_set = []
    new_patients = []
    new_masks = []
    n_trimmed_pts = 0
    print("\nDISCARDED DATA POINTS BASED ON '{}':".format(sort_method))
    for i, (x, y, p, m) in enumerate(zip(x_set, y_set, patients, masks)):
        slices = len(x[0][0])
        dimensions_mask, size = get_size_mask_efficiently(m)
        size_box = np.prod(dimensions_mask)
        trim_it = True
        if sort_method == "slices":
            if slices > val_left and slices < val_right:
                trim_it = False
        elif sort_method == "sizes":
            if size > val_left and size < val_right:
                trim_it = False
        elif sort_method == "sizes_masks":
            if size_box > val_left and size_box < val_right:
                trim_it = False
        else:
            print("Error, only parameters accepted: {}".format(["slices", "sizes", "sizes_masks"]))
            return None
        if not trim_it:
            new_x_set.append(x)
            new_y_set.append(y)
            new_patients.append(p)
            new_masks.append(m)
        else:
            n_trimmed_pts += 1
            print("  {}: Index: {}, Slices: {}, Size: {}, Box Size: {}".format(n_trimmed_pts, i,
                                                                               slices, size,
                                                                               size_box))
    print("\nREMAINING DATA DIMENSIONS:")
    print("  Removed data with '{0}' <= {1}, or '{0}' >= {2}".format(sort_method, val_left,
                                                                     val_right))
    print("  Size new dataset: {}".format(len(new_y_set)))
    return new_x_set, new_y_set, new_patients, new_masks


def calculate_shared_axis(data1, data2, constant_factor=0.05):
    """Get shared axis of data1 and data2: [min(data1, data2), max(data1, data2]]."""
    max_val = max(max(data1), max(data2))
    min_val = min(min(data1), min(data2))
    max_val += (max_val - min_val) * constant_factor
    min_val -= (max_val - min_val) * constant_factor
    return (min_val, max_val)


def analyze_data(volumes, labels, patients, masks, plot_data=True, initial_figure=0, suffix="",
                 title_suffix="", dataset_name="organized", allow_less_3_slices=False):
    """Print statistics of data and maybe plot them if plot_data is True."""
    num_labels = [0, 0]
    sizes_masks = [np.zeros(3), np.zeros(3)]
    all_sizes_masks = [[], []]
    all_sizes = [[], []]
    all_slices = [[], []]
    abs_num_slices = []
    for label, patient, volume, mask in zip(labels, patients, volumes, masks):
        if volume.shape[2] < 3 and not allow_less_3_slices:
            continue  # patient ignored, it is too small
        size_mask, granular_volume = get_size_mask_efficiently(mask)
        abs_num_slices.append(size_mask[2])
        if abs_num_slices[-1] < 3:
            continue  # patient ignored, it is too small (but we have added it to abs_num_slices)
        all_sizes[label].append(granular_volume)
        sizes_masks[label] += size_mask
        all_sizes_masks[label].append(np.prod(size_mask))
        all_slices[label].append(volume.shape[2])
        num_labels[label] += 1
    sizes_masks_mean = [s/n for s, n in zip(sizes_masks, num_labels)]
    print("\nNumber patients:", num_labels[0] + num_labels[1])
    print(" ")
    print("LABEL 1")
    print("  NUMBER SAMPLES: {}".format(num_labels[1]))
    print("  TUMOR BOX VOLUME (px^3 of tumor box)")
    print("    Mean:     {} (in 3 directions: {})".format(np.prod(sizes_masks_mean[1]),
                                                          sizes_masks_mean[1]))
    print("    Median:   {}".format(np.median(all_sizes_masks[1])))
    print("    Variance: {}".format(np.var(all_sizes_masks[1])))
    print("    Min:      {}".format(np.min(all_sizes_masks[1])))
    print("    Max:      {}".format(np.max(all_sizes_masks[1])))
    print("  NUMBER SLICES")
    print("    Mean:     {}".format(np.mean(all_slices[1])))
    print("    Median:   {}".format(np.median(all_slices[1])))
    print("    Variance: {}".format(np.var(all_slices[1])))
    print("    Min:      {}".format(np.min(all_slices[1])))
    print("    Max:      {}".format(np.max(all_slices[1])))
    print("  GRANULAR VOLUME (px^3 that were labeled as tumor)")
    print("    Mean:     {}".format(np.mean(all_sizes[1])))
    print("    Median:   {}".format(np.median(all_sizes[1])))
    print("    Variance: {}".format(np.var(all_sizes[1])))
    print("    Min:      {}".format(np.min(all_sizes[1])))
    print("    Max:      {}".format(np.max(all_sizes[1])))
    print(" ")
    print("LABEL 0")
    print("  NUMBER SAMPLES: {}".format(num_labels[0]))
    print("  TUMOR BOX VOLUME (px^3 of tumor box)")
    print("    Mean:     {} (in 3 directions: {})".format(np.prod(sizes_masks_mean[0]),
                                                          sizes_masks_mean[0]))
    print("    Median:   {}".format(np.median(all_sizes_masks[0])))
    print("    Variance: {}".format(np.var(all_sizes_masks[0])))
    print("    Min:      {}".format(np.min(all_sizes_masks[0])))
    print("    Max:      {}".format(np.max(all_sizes_masks[0])))
    print("  NUMBER SLICES")
    print("    Mean:     {}".format(np.mean(all_slices[0])))
    print("    Median:   {}".format(np.median(all_slices[0])))
    print("    Variance: {}".format(np.var(all_slices[0])))
    print("    Min:      {}".format(np.min(all_slices[0])))
    print("    Max:      {}".format(np.max(all_slices[0])))
    print("  GRANULAR VOLUME (px^3 that were labeled as tumor)")
    print("    Mean:     {}".format(np.mean(all_sizes[0])))
    print("    Median:   {}".format(np.median(all_sizes[0])))
    print("    Variance: {}".format(np.var(all_sizes[0])))
    print("    Min:      {}".format(np.min(all_sizes[0])))
    print("    Max:      {}".format(np.max(all_sizes[0])))
    print(" ")
    if plot_data:
        plt.ion()
    num_bins = 20
    f = initial_figure
    xlim = calculate_shared_axis(all_slices[0], all_slices[1])
    plot_histogram(all_slices[0], "Slices 0", f, 311, num_bins, xlim, show=plot_data,
                   close_all=True)
    plot_histogram(all_slices[1], "Slices 1", f, 312, num_bins, xlim, show=plot_data)
    plot_histogram(all_slices[0] + all_slices[1], "Slices Total", f, 313, num_bins, xlim,
                   window_title="Slices " + title_suffix, show=plot_data)
    f += 1
    xlim = calculate_shared_axis(all_sizes[0], all_sizes[1])
    plot_histogram(all_sizes[0], "Sizes 0", f, 311, num_bins, xlim, show=plot_data)
    plot_histogram(all_sizes[1], "Sizes 1", f, 312, num_bins, xlim, show=plot_data)
    plot_histogram(all_sizes[0] + all_sizes[1], "Sizes Total", f, 313, num_bins, xlim,
                   window_title="Sizes " + title_suffix, show=plot_data)
    f += 1
    xlim = calculate_shared_axis(all_sizes_masks[0], all_sizes_masks[1])
    plot_histogram(all_sizes_masks[0], "Sizes Box 0", f, 311, num_bins, xlim, show=plot_data)
    plot_histogram(all_sizes_masks[1], "Sizes Box 1", f, 312, num_bins, xlim, show=plot_data)
    plot_histogram(all_sizes_masks[0] + all_sizes_masks[1], "Sizes Box Total", f, 313,
                   num_bins, xlim, window_title="Sizes Box " + title_suffix, show=plot_data)
    f += 1
    ylim = calculate_shared_axis(all_slices[0], all_slices[1])
    plot_boxplot(all_slices[0], "Slices 0", f, 121, ylim, show=plot_data)
    plot_boxplot(all_slices[1], "Slices 1", f, 122, ylim, True, show=plot_data,
                 window_title="Slices " + title_suffix)
    f += 1
    ylim = calculate_shared_axis(all_sizes[0], all_sizes[1])
    plot_boxplot(all_sizes[0], "Sizes 0", f, 121, ylim, show=plot_data)
    plot_boxplot(all_sizes[1], "Sizes 1", f, 122, ylim, True, show=plot_data,
                 window_title="Sizes " + title_suffix)
    f += 1
    ylim = calculate_shared_axis(all_sizes_masks[0], all_sizes_masks[1])
    plot_boxplot(all_sizes_masks[0], "Sizes box 0", f, 121, ylim, show=plot_data)
    plot_boxplot(all_sizes_masks[1], "Sizes box 1", f, 122, ylim, True, show=plot_data,
                 window_title="Sizes Box " + title_suffix)
    # Save PDF results
    save_plt_figures_to_pdf("data/{}/results{}.pdf".format(dataset_name, suffix))
    if plot_data:
        input("Press ENTER to close all figures and continue.")
        plt.close("all")
        plt.ioff()
    return ((num_labels[0], num_labels[1]), (np.median(all_slices[0]), np.median(all_slices[1])),
            (all_slices, all_sizes, all_sizes_masks, abs_num_slices))


def get_distances_from_box_size(size_box):
    """Return the distance from center box to top-left and bottom-right corners."""
    # If any number n in size_box is even, the size used will be n-1
    sbpos = []
    sbneg = []
    sboffset = []
    for i in range(len(size_box)):
        if size_box[i] is None:
            sbpos.append(None)
            sbneg.append(None)
            sboffset.append(0)
        else:
            sbpos.append(int((size_box[i] - 1) / 2))
            sbneg.append(-sbpos[-1])
            sboffset.append(sbpos[-1])
    return sbpos, sbneg, sboffset


def grouped(iterable, n):
    """Group iterable in groups of n elements. i.e. ([1,2,3,4], 2) becomes [1,2],[3,4]."""
    return zip(*[iter(iterable)]*n)


def get_all_centers_of_samples(mask, sbpos, sbneg, sboffset, offset_if_None=0):
    """Get position for all 1s in mask that can be the center of a box of size size_box.

    size_bos = None means only the min and max position in that direction will be saved.
    For example, in the following 2D mask:
      [[0, 0, 1, 0, 0, 1],
       [0, 1, 1, 0, 0, 0],
       [0, 1**, 0, 0, 0, 1**],
       [1**, 1, 0, 1*, 0, 0],
       [1, 0, 1, 1, 0, 1],
       [0, 0, 0, 1, 1, 1]]
    with a size_box of (5, 5) we get only one center: [3, 3]*.
    with a size_box of (5, None) we get only one center: ([2, 1], [2, 5]), ([3, 0], [3, 3])**.
    Warning! Won't work well if there is more than one None.
    """
    if len(sbpos) >= 3:
        r = np.transpose(np.nonzero(mask[sbpos[0]:sbneg[0], sbpos[1]:sbneg[1], sbpos[2]:sbneg[2]]))
    else:
        r = np.transpose(np.nonzero(mask[sbpos[0]:sbneg[0], sbpos[1]:sbneg[1]]))
    r += sboffset
    none_found = False
    for i in range(len(sbpos)):
        if sbpos[i] is not None:
            continue
        none_found = True
        d1 = {}
        d2 = {}
        new_r = []
        n = mask.shape[i] - offset_if_None
        for coords in r:
            if coords[i] < offset_if_None or coords[i] >= n:
                continue
            c = tuple(np.append(coords[:i], coords[i + 1:]))
            if c not in d1:
                d1[c] = len(new_r)
                new_r.append(coords)
            elif new_r[d1[c]][i] > coords[i]:
                new_r[d1[c]] = coords
            if c not in d2:
                d2[c] = len(new_r)
                new_r.append(coords)
            elif new_r[d2[c]][i] < coords[i]:
                new_r[d2[c]] = coords
        r = np.array(new_r)
    if none_found:
        return np.array(list(grouped(r, 2)))
    return r


def get_sample_from_center(volume, center, sbpos, sbneg):
    """Return box in volume at pos center with size size_box. No value in center can be None."""
    return volume[center[0] + sbneg[0]:center[0] + sbpos[0] + 1,
                  center[1] + sbneg[1]:center[1] + sbpos[1] + 1,
                  center[2] + sbneg[2]:center[2] + sbpos[2] + 1]


def get_sample_from_center_special(volume, center, sbpos, sbneg):
    """Return box in volume at pos center with size size_box. One value in center must be None.

    center here is a pair of values, and we return all slices from center[0] to center[1].
    """
    pos = []
    neg = []
    for i in range(len(sbpos)):
        if sbpos[i] is None:
            pos.append(0)
            neg.append(0)
        else:
            pos.append(sbpos[i])
            neg.append(sbneg[i])
    c0 = center[0]
    c1 = center[1]
    return volume[c0[0] + neg[0]:c1[0] + pos[0] + 1,
                  c0[1] + neg[1]:c1[1] + pos[1] + 1,
                  c0[2] + neg[2]:c1[2] + pos[2] + 1]


def interpolate_data(x_set, masks, pixels_separation):
    """Interpolate data to make pixels have same separation in all directions (x, y, z)."""
    pixel_side = min(pixels_separation)
    print("\nInterpolating. This may take a few minutes...")
    for i in range(len(x_set)):
        print("{}/{}".format(i + 1, len(x_set)))
        min_coord = np.array([0, 0, 0])
        max_coord = np.multiply(np.array(x_set[i]).shape, pixels_separation)
        x = np.arange(min_coord[0], max_coord[0], pixels_separation[0])
        y = np.arange(min_coord[1], max_coord[1], pixels_separation[1])
        z = np.arange(min_coord[2], max_coord[2], pixels_separation[2])
        max_coord = [max(x) + 0.01, max(y) + 0.01, max(z) + 0.01]
        interpolating_func = RegularGridInterpolator((x, y, z), np.array(x_set[i]))
        rangex = np.arange(min_coord[0], max_coord[0], pixel_side)
        rangey = np.arange(min_coord[1], max_coord[1], pixel_side)
        rangez = np.arange(min_coord[2], max_coord[2], pixel_side)
        xlist = np.zeros((len(rangex), len(rangey), len(rangez)))
        new_mask = np.zeros(xlist.shape)
        mask = masks[i]
        # Unfortunately, I am doing this manually, I could not figure out how to do it with numpy
        for ii, xi in enumerate(rangex):
            posx = xi / pixels_separation[0]
            idx0x = int(posx)
            idx1x = int(np.ceil(posx))
            diff0x = 1 - posx + idx0x
            diff1x = 1 - diff0x
            for jj, yi in enumerate(rangey):
                posy = yi / pixels_separation[1]
                idx0y = int(posy)
                idx1y = int(np.ceil(posy))
                diff0y = 1 - posy + idx0y
                diff1y = 1 - diff0y
                for kk, zi in enumerate(rangez):
                    xlist[ii, jj, kk] = interpolating_func([xi, yi, zi])[0]
                    posz = zi / pixels_separation[2]
                    idx0z = int(posz)
                    idx1z = int(np.ceil(posz))
                    diff0z = 1 - posz + idx0z
                    diff1z = 1 - diff0z
                    value = (mask[idx0x, idx0y, idx0z] * diff0x * diff0y * diff0z +
                             mask[idx0x, idx0y, idx1z] * diff0x * diff0y * diff1z +
                             mask[idx0x, idx1y, idx0z] * diff0x * diff1y * diff0z +
                             mask[idx0x, idx1y, idx1z] * diff0x * diff1y * diff1z +
                             mask[idx1x, idx0y, idx0z] * diff1x * diff0y * diff0z +
                             mask[idx1x, idx0y, idx1z] * diff1x * diff0y * diff1z +
                             mask[idx1x, idx1y, idx0z] * diff1x * diff1y * diff0z +
                             mask[idx1x, idx1y, idx1z] * diff1x * diff1y * diff1z)
                    new_mask[ii, jj, kk] = int(np.round(value))
        x_set[i] = xlist
        masks[i] = new_mask
    return x_set, masks


def calculate_samples_per_label(x_set_train, y_set_train, masks_train, patients_by_label_train,
                                results_train, x_set_test, y_set_test, masks_test,
                                patients_by_label_test, results_test, num_samples, size_box,
                                offset_if_None):
    """Get number of samples that must be extracted from every patient so labels are balanced.

    If a number in size_box is None, all the slices in that direction except the most exterior
    offset_if_None layers will be used for one box.
    """
    # Convert size box (i.e. (5,5,None)) to (2,2,None), (-2,-2,None), (2,2,0)
    size_box_params = get_distances_from_box_size(size_box)
    # Get position all centers: positions in a patient that can be the center of a box
    center_samples_train = []  # Center of all boxes with size_box size in x_set[i]
    min_num_samples_train = [None, None]
    for i, (mask, label, volume) in enumerate(zip(masks_train, y_set_train, x_set_train)):
        center_samples_train.append(get_all_centers_of_samples(mask, *size_box_params,
                                                               offset_if_None))
        if volume.shape[2] < 3 or results_train[3][i] < 3:
            continue  # patient ignored, it is too small
        try:
            min_num_samples_train[label] = min(len(center_samples_train[-1]),
                                               min_num_samples_train[label])
        except TypeError:
            min_num_samples_train[label] = len(center_samples_train[-1])
    center_samples_test = []  # Center of all boxes with size_box size in x_set[i]
    min_num_samples_test = [None, None]
    for i, (mask, label, volume) in enumerate(zip(masks_test, y_set_test, x_set_test)):
        center_samples_test.append(get_all_centers_of_samples(mask, *size_box_params,
                                                              offset_if_None))
        if volume.shape[2] < 3 or results_test[3][i] < 3:
            continue  # patient ignored, it is too small
        try:
            min_num_samples_test[label] = min(len(center_samples_test[-1]),
                                              min_num_samples_test[label])
        except TypeError:
            min_num_samples_test[label] = len(center_samples_test[-1])
    # Adapt the number of samples per patient for one of the labels so that the labels get balanced
    if num_samples is not None:
        min_num_samples_train[0] = num_samples
        min_num_samples_train[1] = num_samples
        min_num_samples_test[0] = num_samples
        min_num_samples_test[1] = num_samples
    num_patients_by_label = [patients_by_label_train[0] + patients_by_label_test[0],
                             patients_by_label_train[1] + patients_by_label_test[1]]
    min_num_samples = [min(min_num_samples_train[0], min_num_samples_test[0]),
                       min(min_num_samples_train[1], min_num_samples_test[1])]
    factor = (num_patients_by_label[0] * min_num_samples[0] /
              num_patients_by_label[1] / min_num_samples[1])
    if factor > 1:
        min_num_samples[0] = int(np.ceil(min_num_samples[0] / factor))
    else:
        min_num_samples[1] = int(np.ceil(min_num_samples[1] * factor))
    return min_num_samples, center_samples_train, center_samples_test


def resample_volumes(x_set, y_set, patients, masks, num_samples, size_box, center_samples,
                     min_num_samples, results):
    """Resample volumes to make boxes of constant size size_box."""
    # Convert size box (i.e. (5,5,None)) to (2,2,None), (-2,-2,None), (2,2,0)
    size_box_params = get_distances_from_box_size(size_box)
    # Cut volumes and resample
    new_x = []
    new_y = []
    new_patients = []
    new_masks = []
    special = None in size_box
    for i, (volume, mask, label, patient, samples) in enumerate(zip(x_set, masks,
                                                                    y_set, patients,
                                                                    center_samples)):
        if volume.shape[2] < 3 or results[3][i] < 3:
            continue  # patient ignored, it is too small
        min_different_samples = min_num_samples[label]
        k = 0
        while min_different_samples > 0:
            pos = np.random.permutation(np.arange(len(samples)))[:min_different_samples]
            min_different_samples -= len(samples)
            for j, center in enumerate(samples[pos]):
                if not special:
                    new_x.append(get_sample_from_center(volume, center,
                                                        *size_box_params[:2]))
                    new_masks.append(get_sample_from_center(mask, center,
                                                            *size_box_params[:2]))
                else:
                    new_x.append(get_sample_from_center_special(volume, center,
                                                                *size_box_params[:2]))
                    new_masks.append(get_sample_from_center_special(mask, center,
                                                                    *size_box_params[:2]))
                new_y.append(label)
                # new_patients.append(patient + "_{:06}".format(j + k * len(pos)))
                new_patients.append(patient)  # patient names must be mantained!
            k += 1
    return new_x, new_y, new_patients, new_masks


def get_bucket(bucket0, bucket1, ratio=0.5):
    """Get size of two buckets and tell what bucket to put next obj to get closer to buckets ratio.

    For example, if buckets=[2, 3] and ratio=0.5, returns 0 to get [3, 3],
    but if ratio=0.66 returns 1 to obtain [2, 4]
    Assumes only 2 buckets, will ignore any other buckets
    Assumes numbers in buckets >= 0
    """
    try:
        current_ratio = bucket0 / (bucket0 + bucket1)
    except ZeroDivisionError:
        current_ratio = 0
    return 0 if current_ratio < ratio else 1


def normalize_3D_volumes(volumes):
    """Normalize volume, so minimum value in it is 0 and maximum value is 1."""
    for i, volume in enumerate(volumes):
        maxv = np.max(volume)
        minv = np.min(volume)
        volumes[i] = (volume - minv) / (maxv - minv)
    return volumes


def generate_2D_dataset(samples, labels, patients, masks, slices_per_sample=3, rotate_data=False,
                        normalize=True):
    """From 3D volumes generates a 2D dataset."""
    counter = 0
    rotations = 4 if rotate_data else 1
    x_dataset = []
    y_dataset = []
    masks_dataset = []
    patients_dataset = []
    for volume, label, patient, mask in zip(samples, labels, patients, masks):
        # Normalize values from 0 to 1
        if normalize:
            volume = normalize_3D_volumes([volume])[0]
        for r in range(rotations):
            # If rotations=4, rotate image 4 times (0, 90, 180 and 270 deg) to increase sample size
            vol = np.rot90(volume, k=r)
            msk = np.rot90(mask, k=r)
            for idx in range(vol.shape[2] - slices_per_sample + 1):
                image = vol[:, :, idx:idx + slices_per_sample]
                mask_image = msk[:, :, idx:idx + slices_per_sample]
                x_dataset.append(image)
                y_dataset.append(label)
                masks_dataset.append(mask_image)
                patients_dataset.append(patient + (str(r * 90) if r != 0 else ""))
        counter += 1
        print("{}  {} / {} patients processed".format(get_current_time(microseconds=True),
                                                      counter, len(patients)))
    x_dataset = np.array(x_dataset)
    y_dataset = np.array(y_dataset)
    masks_dataset = np.array(masks_dataset)
    return x_dataset, y_dataset, patients_dataset, masks_dataset


def save_dataset_correctly(x, y, patients, masks, parent_folder="data", dataset_name="organized",
                           dataset_subname="training_set"):
    """Save data in a not naive way, balancing labels and medians in the training and test set."""
    # Create folder data if it does not exist
    full_path = parent_folder + "/"
    try:
        os.mkdir(full_path)
    except OSError:
        pass
    folder_path = full_path + dataset_name + "/"
    try:
        os.mkdir(folder_path)
    except OSError:
        pass
    file_path = "{}{}".format(folder_path, dataset_subname)
    try:
        np.savez(file_path, x=x, y=y)
        print("Dataset saved in: '{}.npz'".format(file_path))
        print("Shape X:    {}".format(x.shape))
        print("Shape Y:    {}".format(y.shape))
    except ValueError:
        with open("{}.pkl".format(file_path), "wb") as f:
            pickle.dump(x, f)
            pickle.dump(y, f)
        print("Error while saving set as '.npz'. Dataset saved in: '{}.pkl'".format(file_path))
    with open("{}_patients.pkl".format(file_path), "wb") as f:
        pickle.dump(patients, f)
    print("Patients saved in '{}_patients.pkl'.".format(file_path))
    try:
        np.savez("{}_masks".format(file_path), masks=masks)
        print("Masks saved in: '{}_masks.npz'".format(file_path))
        print("Shape Mask: {}".format(masks.shape))
    except ValueError:
        with open("{}_masks.pkl".format(file_path), "wb") as f:
            pickle.dump(masks, f)
        print("Masks saved in '{}_masks.pkl'.".format(file_path))


def improved_save_data(x_set, y_set, patients, masks, dataset_name="organized",
                       plot_data=False, trim_data=True, data_interpolation=None, normalize=True,
                       convert_to_2d=True, resampling=None, skip_dialog=False, plot_slices=False):
    """Save dataset so labels & slices medians are equally distributed in training and test set."""
    # Add suffixes ta dataset name, so it is easy to know how every dataset was generated
    if not convert_to_2d:
        dataset_name += "_3d"
    if not normalize:
        dataset_name += "_unnormalized"
    if trim_data:
        # 2 represents that we are using sizes (2) to trim, not slices (1), or box_sizes (3)
        trim_option = 2
        dataset_name += "_trimmed{}".format(trim_option)
    if data_interpolation is not None:
        dataset_name += "_interpolated"
    if resampling is not None:
        dataset_name += "_resampled"
    print("\nOutput location: 'data/{}/'".format(dataset_name))

    if plot_slices:
        plt.ion()
        for i, (x, y, p, m) in enumerate(zip(x_set, y_set, patients, masks)):
            plot_pet_slice(x, center=int(x.shape[2] / 2), mask=m, figure=99,
                           label="patient {} - label {}".format(p, y))
        plt.ioff()
        plt.close("all")

    # Analyze data and plot some statistics
    print("\nAnalyzing data...")
    num_patients_by_label, medians_by_label, results = analyze_data(x_set, y_set, patients, masks,
                                                                    plot_data=plot_data,
                                                                    dataset_name=dataset_name)
    if trim_data:
        slices, sizes, box_sizes, abs_num_slices = results
        slices = slices[0] + slices[1]
        sizes = sizes[0] + sizes[1]
        box_sizes = box_sizes[0] + box_sizes[1]
        if trim_option == 1:
            # Trim based on the number of slices
            print("Trimming based on slices...")
            x_set1, y_set1, patients1, masks1 = trim_edges(slices, "slices", x_set, y_set,
                                                           patients, masks, trim_pos=(0.1, 0.9))
            print("Analyzing data...")
            params = analyze_data(x_set1, y_set1, patients1, masks1, plot_data=plot_data,
                                  initial_figure=6, suffix="_trimmed_slices",
                                  title_suffix="(Trimmed Slices)", dataset_name=dataset_name)
            num_patients_by_label1, medians_by_label1, results1 = params
        elif trim_option == 2:
            # Trim based on the tumor volumes (the number of cubic pixels in the mtv contour)
            print("Trimming based on sizes...")
            x_set2, y_set2, patients2, masks2 = trim_edges(sizes, "sizes", x_set, y_set, patients,
                                                           masks, trim_pos=(0.11, 0.89))
            print("Analyzing data...")
            params = analyze_data(x_set2, y_set2, patients2, masks2, plot_data=plot_data,
                                  initial_figure=12, suffix="_trimmed_sizes",
                                  title_suffix="(Trimmed Sizes)", dataset_name=dataset_name)
            num_patients_by_label2, medians_by_label2, results2 = params
        elif trim_option == 3:
            # Trim based on the size of the boxs containing the contour
            print("Trimming based on box sizes...")
            x_set3, y_set3, patients3, masks3 = trim_edges(box_sizes, "sizes_masks", x_set, y_set,
                                                           patients, masks, trim_pos=(0.11, 0.89))
            print("Analyzing data...")
            params = analyze_data(x_set3, y_set3, patients3, masks3, plot_data=plot_data,
                                  initial_figure=18, suffix="_trimmed_box_sizes",
                                  title_suffix="(Trimmed Box Sizes)", dataset_name=dataset_name)
            num_patients_by_label3, medians_by_label3, results3 = params
        # Use trimed data based on trim_option as dataset
        if trim_option == 1:
            x_set, y_set, patients, masks = x_set1, y_set1, patients1, masks1
            medians_by_label, results = medians_by_label1, results1
            num_patients_by_label = num_patients_by_label1
        elif trim_option == 2:
            x_set, y_set, patients, masks = x_set2, y_set2, patients2, masks2
            medians_by_label, results = medians_by_label2, results2
            num_patients_by_label = num_patients_by_label2
        elif trim_option == 3:
            x_set, y_set, patients, masks = x_set3, y_set3, patients3, masks3
            medians_by_label, results = medians_by_label3, results3
            num_patients_by_label = num_patients_by_label3

    if data_interpolation is not None:
        # Adjust slices so that all pixels are the same width, length and height
        x_set, masks = interpolate_data(x_set, masks, data_interpolation)
        params = analyze_data(x_set, y_set, patients, masks, plot_data=plot_data,
                              initial_figure=24, suffix="_interpolated_slices",
                              title_suffix="(Interpolated Slices)", dataset_name=dataset_name)
        num_patients_by_label, medians_by_label, results = params

    if normalize:
        # Normalize every volume of every patient so the max pixel is 1 and the min pixel is 0
        x_set = normalize_3D_volumes(x_set)
        params = analyze_data(x_set, y_set, patients, masks, plot_data=plot_data,
                              initial_figure=30, suffix="_normalized",
                              title_suffix="(Normalized)", dataset_name=dataset_name)
        num_patients_by_label, medians_by_label, results = params

    # Resampling used to be here, but then samples get mixed between training and test set
    """
    if resampling is not None:
        size_box, num_samples = resampling
        if type(size_box) == int:
            offset_if_None = int((size_box - 1) / 2)
            if convert_to_2d:
                size_box = (size_box, size_box, None)
            else:
                size_box = (size_box, size_box, size_box)
        else:
            offset_if_None = size_box[0]
        params = resample_volumes(x_set, y_set, patients, masks, num_samples, size_box,
                                  offset_if_None, num_patients_by_label, results)
        x_set, y_set, patients, masks = params
        params = analyze_data(x_set, y_set, patients, masks, plot_data=plot_data,
                              initial_figure=30, suffix="_resampled", allow_less_3_slices=True,
                              title_suffix="(Resampled)", dataset_name=dataset_name)
        num_patients_by_label, medians_by_label, results = params
    """

    # After analyze_data, if we do not trim, interpolate, or resample it, we have 77 patients
    # Label 1: NUMBER SAMPLES: 20
    #          MEAN TUMOR BOX VOLUME: 3174
    #          MEAN NUMBER SLICES: 17
    #          MEAN GRANULAR VOLUME: 867
    # LABEL 0: NUMBER SAMPLES: 57
    #          MEAN TUMOR BOX VOLUME: 1440
    #          MEAN NUMBER SLICES: 11
    #          MEAN GRANULAR VOLUME: 486
    # Based on this data, we sample the dataset in the best possible way so all kinds of data is
    # represented in the train and test set. We split data in in 4 groups: label 0 / label 1, and
    # above / below median, and put an equal percentage of everything in train and test set
    if not trim_data:
        train_to_total_ratio = 63 / 77  # 77 patients, let's do training+validation = 63, test = 14
    else:
        train_to_total_ratio = 0.1
    train_nums = [[0, 0], [0, 0]]
    test_nums = [[0, 0], [0, 0]]

    test_set_x = []
    train_set_x = []
    test_set_y = []
    train_set_y = []
    test_set_patients = []
    train_set_patients = []
    test_set_masks = []
    train_set_masks = []

    # Distribute data in train and test set
    median_indices = [[], []]
    for i, (label, patient, volume, mask) in enumerate(zip(y_set, patients, x_set, masks)):
        num_slices = volume.shape[2]
        if num_slices < 3:
            continue  # patient ignored, it is too small
        abs_num_slices = results[3][i]  # Counts number slices that have at least 1 contour pixel
        if abs_num_slices < 3:
            continue  # patient ignored, it is too small
        if num_slices < medians_by_label[label]:
            if get_bucket(train_nums[0][label], test_nums[0][label], train_to_total_ratio) == 0:
                train_nums[0][label] += 1
                train_set_x.append(volume)
                train_set_y.append(label)
                train_set_patients.append(patient)
                train_set_masks.append(mask)
            else:
                test_nums[0][label] += 1
                test_set_x.append(volume)
                test_set_y.append(label)
                test_set_patients.append(patient)
                test_set_masks.append(mask)
        elif num_slices > medians_by_label[label]:
            if get_bucket(train_nums[1][label], test_nums[1][label], train_to_total_ratio) == 0:
                train_nums[1][label] += 1
                train_set_x.append(volume)
                train_set_y.append(label)
                train_set_patients.append(patient)
                train_set_masks.append(mask)
            else:
                test_nums[1][label] += 1
                test_set_x.append(volume)
                test_set_y.append(label)
                test_set_patients.append(patient)
                test_set_masks.append(mask)
        else:
            median_indices[label].append(i)
    for label, indices in enumerate(median_indices):
        for index in indices:
            volume = x_set[index]
            patient = patients[index]
            mask = masks[index]
            if get_bucket(train_nums[0][label] + train_nums[1][label],
                          test_nums[0][label] + test_nums[1][label], train_to_total_ratio) == 0:
                train_nums[0][label] += 1
                train_set_x.append(volume)
                train_set_y.append(label)
                train_set_patients.append(patient)
                train_set_masks.append(mask)
            else:
                test_nums[0][label] += 1
                test_set_x.append(volume)
                test_set_y.append(label)
                test_set_patients.append(patient)
                test_set_masks.append(mask)
    ratio = abs(len(train_set_x) / (len(train_set_x) + len(test_set_x)) - train_to_total_ratio)
    ratio0 = abs((len(train_set_x) + 1) /
                 (len(train_set_x) + len(test_set_x)) - train_to_total_ratio)
    ratio1 = abs((len(train_set_x) - 1) /
                 (len(train_set_x) + len(test_set_x)) - train_to_total_ratio)
    if ratio0 < ratio:
        train_set_x.append(test_set_x.pop())
        train_set_y.append(test_set_y.pop())
        train_set_patients.append(test_set_patients.pop())
        train_set_masks.append(test_set_masks.pop())
    elif ratio1 < ratio:
        test_set_x.append(train_set_x.pop())
        test_set_y.append(train_set_y.pop())
        test_set_patients.append(train_set_patients.pop())
        test_set_masks.append(train_set_masks.pop())

    # Plot and save results
    params1 = analyze_data(train_set_x, train_set_y, train_set_patients, train_set_masks,
                           plot_data=plot_data, initial_figure=36, suffix="_train_set",
                           title_suffix="(Train Set)", dataset_name=dataset_name)
    params2 = analyze_data(test_set_x, test_set_y, test_set_patients, test_set_masks,
                           plot_data=plot_data, initial_figure=42, suffix="_test_set",
                           title_suffix="(Test Set)", dataset_name=dataset_name)

    # Resample after splitting data in training and test set, to avoid putting samples in training
    # set from test set and viceversa
    if resampling is not None:
        size_box, num_samples = resampling
        if type(size_box) == int:
            offset_if_None = int((size_box - 1) / 2)
            if convert_to_2d:
                size_box = (size_box, size_box, None)
            else:
                size_box = (size_box, size_box, size_box)
        else:
            offset_if_None = size_box[0]
        # Get number of samples per patient (depending on label) and centers of all samples
        num_patients_by_label_train, medians_by_label_train, results_train = params1
        num_patients_by_label_test, medians_by_label_test, results_test = params2
        params = calculate_samples_per_label(train_set_x, train_set_y, train_set_masks,
                                             num_patients_by_label_train, results_train,
                                             test_set_x, test_set_y, test_set_masks,
                                             num_patients_by_label_test, results_test,
                                             num_samples, size_box, offset_if_None)
        samples_per_label, centers_train, centers_test = params
        # Training set
        params = resample_volumes(train_set_x, train_set_y, train_set_patients, train_set_masks,
                                  num_samples, size_box, centers_train, samples_per_label,
                                  results_train)
        train_set_x, train_set_y, train_set_patients, train_set_masks = params
        params3 = analyze_data(train_set_x, train_set_y, train_set_patients, train_set_masks,
                               plot_data=plot_data, initial_figure=48, dataset_name=dataset_name,
                               suffix="_train_set_resampled", title_suffix="(Train Set Resampled)")
        # Test set
        params = resample_volumes(test_set_x, test_set_y, test_set_patients, test_set_masks,
                                  num_samples, size_box, centers_test, samples_per_label,
                                  results_test)
        test_set_x, test_set_y, test_set_patients, test_set_masks = params
        params4 = analyze_data(test_set_x, test_set_y, test_set_patients, test_set_masks,
                               plot_data=plot_data, initial_figure=54, dataset_name=dataset_name,
                               suffix="_test_set_resampled", title_suffix="(Test Set Resampled)")
        # Train set and test set together
        params5 = analyze_data(train_set_x + test_set_x, train_set_y + test_set_y,
                               train_set_patients + test_set_patients,
                               train_set_masks + test_set_masks,
                               plot_data=plot_data, initial_figure=60, dataset_name=dataset_name,
                               suffix="_whole_set_resampled", title_suffix="(Whole Set Resampled)")

    # Print results
    print("\nDATASET DIVIDED IN TRAINING AND TEST SET")
    print("  TRAINING SET")
    print("    Number of samples: {}".format(len(train_set_x)))
    print("    Label frequency: {}".format(dict(Counter(train_set_y))))
    print("  TEST SET")
    print("    Number of samples: {}".format(len(test_set_x)))
    print("    Label frequency: {}\n".format(dict(Counter(test_set_y))))

    # Possibly convert 3D dataset into 2D dataset and save data
    answer = ""
    while not skip_dialog and (len(answer) <= 0 or answer[0].strip().lower() != "y"):
        print("Are you sure you want to save? This may overwrite some files.")
        answer = input("Type 'y' to save data or Ctrl-C to abort.\n>> ")
    print(" ")
    if convert_to_2d:
        train_data = generate_2D_dataset(train_set_x, train_set_y, train_set_patients,
                                         train_set_masks, normalize=False)
        x, y, patients_dataset, masks_dataset = train_data
        print(" ")
    else:
        x, y = train_set_x, train_set_y
        patients_dataset, masks_dataset = train_set_patients, train_set_masks
    save_dataset_correctly(x, y, patients_dataset, masks_dataset,
                           dataset_name=dataset_name, dataset_subname="training_set")
    print(" ")
    if convert_to_2d:
        test_data = generate_2D_dataset(test_set_x, test_set_y, test_set_patients, test_set_masks,
                                        normalize=False)  # Volumes already normalized
        x, y, patients_dataset, masks_dataset = test_data
        print(" ")
    else:
        x, y = test_set_x, test_set_y
        patients_dataset, masks_dataset = test_set_patients, test_set_masks
    save_dataset_correctly(x, y, patients_dataset, masks_dataset,
                           dataset_name=dataset_name, dataset_subname="test_set")


def parse_arguments(suffix=""):
    """Parse arguments in code."""
    parser = argparse.ArgumentParser(description="This code requires the files "
                                     "'dataset{0}_images.pkl', 'dataset{0}_labels.pkl', "
                                     "'dataset{0}_patients.pkl', and 'dataset{0}_masks.pkl' "
                                     "(which are generated by the program "
                                     "'parse_volumes_dataset.py'). From such files it generates "
                                     "a training_set and a test_set .npz files and other .pkl "
                                     "files that go with them (maps of the patient names and "
                                     "masks), which are saved in the folder './data/organized'."
                                     " It performs some transformation on the data, like "
                                     "rearranging it to make sure the same percentage of 0 and 1 "
                                     "labels are on both the training and test set, and that the "
                                     "same percentage of patients with a number of slices above "
                                     "and below the median for label 0 and 1 are on both the train"
                                     " and test set. It also separates every 3D volumes in 3 "
                                     "channels 2D slices. It can also take 'lumpy' files "
                                     "generated by 'generate_dataset.py' (see -l argument)."
                                     "".format(suffix))
    parser.add_argument('-p', '--plot', default=False, action="store_true",
                        help="show figures before saving them")
    parser.add_argument('-ps', '--plot_slices', default=False, action="store_true",
                        help="before any transformation or analysis, plot volumes and masks "
                        "for every slice")
    parser.add_argument('-t', '--trim', default=False, action="store_true",
                        help="get rid of outliers: 10%% smaller and larger tumors")
    parser.add_argument('-i', '--interpolate', default=False, action="store_true",
                        help="interpolate volumes so pixels are cubes: the spacing between "
                        "adjacent pixels is the same in all directions")
    parser.add_argument('-r', '--resample', default=False, action="store_true",
                        help="resample volumes to multiple cubes of size 5x5x5 pixels"
                        "adjacent pixels is the same in all directions")
    parser.add_argument('-3d', '--in_3d', default=False, action="store_true",
                        help="save 3d data instead of slicing it in 3 channels 2d images")
    parser.add_argument('-u', '--unnormalized', default=False, action="store_true",
                        help="do not normalize volumes")
    parser.add_argument('-y', '--yes', default=False, action="store_true",
                        help="skip confirmation dialogs, this will overwrite data without asking")
    parser.add_argument('-l', '--lumpy', default=False, action="store_true",
                        help="load lumpy_dataset{0}_images.pkl, lumpy_dataset{0}_labels.pkl, "
                        "etc. instead of dataset{0}_images.pkl, etc. (lumpy files are "
                        "normally generated by 'generate_dataset.py')".format(suffix))
    parser.add_argument('-m', '--margin', default=False, action="store_true",
                        help="load dataset_m2{0}_images.pkl, dataset_m2{0}_labels.pkl,"
                        " etc. instead of dataset{0}_images.pkl, etc. If -l is active, "
                        "lumpy_dataset_m2{0}_images.pkl is loaded".format(suffix))
    return parser.parse_args()


if __name__ == "__main__":
    # 0: original volume is unchanged (just put into smaller box)
    # 1: volume is cut exactly by contour
    # 2: volume is cut by contour but adding margin of 3 pixels
    dataset_format = 0
    file_suffix = ""
    name_suffix = ""
    old_format = False
    if dataset_format > 0:
        file_suffix = str(dataset_format)
        if dataset_format == 1:
            name_suffix = "_cut"
        elif dataset_format == 2:
            name_suffix = "_margincut"
        else:
            name_suffix = "_unknown"

    args = parse_arguments(suffix=file_suffix)
    file_prefix = "lumpy_" if args.lumpy else ""
    file_suffix = "_m2" + file_suffix if args.margin else file_suffix
    m2 = "m2_" if args.margin else ""
    dataset_name = file_prefix + m2 + "organized" + name_suffix

    # Load data
    print("Reading '{}dataset{}_images.pkl'".format(file_prefix, file_suffix))
    with open('{}dataset{}_images.pkl'.format(file_prefix, file_suffix), 'rb') as f:
        x = pickle.load(f)
    print("Reading '{}dataset{}_labels.pkl'".format(file_prefix, file_suffix))
    with open('{}dataset{}_labels.pkl'.format(file_prefix, file_suffix), 'rb') as f:
        y = pickle.load(f)
    print("Reading '{}dataset{}_patients.pkl'".format(file_prefix, file_suffix))
    with open('{}dataset{}_patients.pkl'.format(file_prefix, file_suffix), 'rb') as f:
        patients = pickle.load(f)
    if not old_format:
        print("Reading '{}dataset{}_masks.pkl'".format(file_prefix, file_suffix))
        with open('{}dataset{}_masks.pkl'.format(file_prefix, file_suffix), 'rb') as f:
            masks = pickle.load(f)
    # Make sure x and y are the same length
    assert(len(x) == len(y))
    assert(len(x) == len(patients))
    if old_format:
        save_data(x, y, patients, number=1, suffix=name_suffix)
    else:
        data_interpolation = None
        if args.interpolate:
            # This is the distance between pixels in the x, y and z direction in our dicom images
            data_interpolation = (4.07283, 4.07283, 5.0)
        resampling = None
        if args.resample:
            # Resample to cubes of 5x5x5 pixels. We can also resample to non-cubic figures, passing
            # (7,4,3) instead of 5 to create a cube of 7x4x3
            # If the second parameter is None, it will find the smallest volume, count how many
            # cubes can be sampled from it, and generate that many samples for all patients. It
            # will also automatically balance labels 0 and 1. Set None to a number to select the
            # number of samples that will be created, sampling randomly.
            resampling = (5, None)
        improved_save_data(x, y, patients, masks, dataset_name=dataset_name, plot_data=args.plot,
                           trim_data=args.trim, data_interpolation=data_interpolation,
                           convert_to_2d=not args.in_3d, resampling=resampling,
                           normalize=not args.unnormalized, skip_dialog=args.yes,
                           plot_slices=args.plot_slices)
